<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Unsupervised Machine Learning</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 1000px;
      margin: 40px auto;
      padding: 0 20px;
    }
    h1, h2, h3 {
      color: #222;
    }
    img {
      display: block;
      margin: 20px auto;
      max-width: 90%;
    }
    pre {
      background: #f8f8f8;
      padding: 10px;
      border-left: 3px solid #ccc;
      overflow-x: auto;
    }
    code {
      font-family: Consolas, monospace;
    }
    section {
      margin-bottom: 40px;
    }
  </style>
</head>

<body>

  <h1>Machine Learning: MNIST Digits</h1>

  <section id="data-overview">
    <h1>Data Overview</h1>
    <p>
        The dataset is readily available within <code>sklearn</code>:
    </p>

<pre><code class="language-python">
import pandas as pd
from sklearn.datasets import fetch_openml

mnist = fetch_openml("mnist_784")
X, y = mnist.data, mnist.target.astype(int)
</code></pre>

    <p>
      <code>X</code> is a dataframe containing 784 features, each representing a pixel in a 28 × 28 image. 
      There are 70,000 observations (digit images) in the dataset. 
      Below is a visualization of 25 randomly sampled observations.
    </p>

    <img src="digits_initial_viz.png" alt="Initial visualization of MNIST digits">
  </section>

  <section id="linear-methods">
    <h1>(Section A) Linear Dimension Reduction Techniques</h1>
    <p>
      This section applies, compares, and contrasts <strong>PCA</strong>, <strong>NMF</strong>, and <strong>ICA</strong>.
    </p>

    <h2>Principal Component Analysis (PCA)</h2>

    <h3> Observations </h3>
    <p>
      PC1 and PC2 capture latent patterns among digits — some digits (e.g., digit 1) are well separated in this projection.
      Interpreting PC1 and PC2 by examining observations along these directions yields the visualization below.
      <img src="pc1_pc2_labeled.png" alt="PCA Scatterplot">
    </p>

    <h3> Features </h3>
    <p>
      PC2 appears to capture <em>digit thickness</em> — moving from positive to negative PC2, digits become thinner.
      PC1 captures a subtler pattern, possibly related to <em>digit slant</em>.
    </p>
    <img src="pc1_pc2_patterns.png" alt="PC Pattern Interpretation">

    <h3> Hyperparameter Tuning </h3>
    The number of principal components (PCs) cannot exceed the number of original features. 
    To determine the optimal number of PCs, we can examine the variance explained using a scree plot and apply the “elbow rule,” 
    or use more quantitative approaches to tune this hyperparameter. 

    Since labels are available, one practical method is to fit a simple multinomial classifier on progressively larger subsets of the principal components 
    and track how classification accuracy changes until it stabilizes. 
    This downstream supervised tuning effectively helps optimize an unsupervised method. 
    Alternatively, we can evaluate the quality of the reduced representation by comparing how well observations in the reduced space preserve relationships 
    from the original space, for example using a Jaccard similarity measure.
    <br><br>
    For PCA, given the analytical closed form, nested, and ordered solution - it makes sense to use supervised optimization. 
    Below I fit 20 mutlinomial classification models on X projected to 5, 10, 15, ..., 100 PCs and evaluate the accuracy of each iteration to inspect convergence.
    </code></pre>
    <img src="hyperparam_tune_supervised_pca.png" alt="PCA Accuracy Curve">

    These results indicate that there is a convergence between 40 to 60 PCs - this appears to be the optimal number of components to use when performing PCA on MNIST dataset. 

    <h2>Non-Negative Matrix Factorization (NMF)</h2>

    <h3> Observations </h3>
    <p>
        Unlike PCA, NMF components are not ordered or nested, so we cannot interpret them sequentially by importance. 
        Below, three arbitrary pairs of components are shown side by side to illustrate how different 2D projections capture various digit groupings without a clear hierarchical structure.
    </p>
    <img src="nmf_1x3_components.png" alt="NMF Observation Scatterplot">
    Because NMF enforces non-negativity, each component contributes additively to the reconstruction of the digits. 
    Unlike PCA, which can capture opposing directions of variation through positive and negative loadings, NMF contains no conflicting variance patterns. 
    This yields more interpretable, part-based components, though at the cost of representing only additive structure in the data.
    Ultimately, the clustering tends to be worse for better interpretation of the patterns.

    <h3> Features </h3>
    <p>
        The image below corresponds to the basis components (the H matrix) learned by NMF. 
        Each component represents a distinct pattern or “building block” that the model has found useful for reconstructing digits. 
        These can be interpreted as localized parts of digits — for example, strokes, loops, or crossbars — that commonly appear across multiple digits.
        The second image shows example digits (observations) that have the highest activation scores for each component, derived from the W matrix. 
        These highlight which digits rely most heavily on each learned part. 
        For instance, component 9 appears to capture the crossbar pattern of the digit 4, while other components align more directly with complete digit shapes such as 1, 0, or 7.
        Overall, NMF decomposes the data into additive, non-negative parts that together reconstruct the full images. 
        While some components resemble entire digits rather than localized features, this behavior is typical for NMF applied to structured image data like MNIST—especially when the number of components is small.
    </p>
    <img src="nmf_basis_features.png" alt="NMF Basis Features">

    <p>
        The image below shows example digits (observations) that have the highest activation scores for each component, derived from the W matrix. 
        These highlight which digits rely most heavily on each learned part. 
        For instance, component 9 appears to capture the crossbar pattern of the digit 4, while other components align more directly with complete digit shapes such as 1, 0, or 7.
        Overall, NMF decomposes the data into additive, non-negative parts that together reconstruct the full images. 
        While some components resemble entire digits rather than localized features, this behavior is typical for NMF applied to structured image data like MNIST—especially when the number of components is small.
    </p>
    <img src="nmf_top_activations_grid.png" alt="NMF Top Activations">
    <h3> Hyperparameter Tuning </h3>
    <p>
        Because NMF must be refit from scratch for each number of components, it is considerably more computationally expensive than PCA. 
        In practice, a coarse grid provides sufficient resolution to identify the accuracy plateau while keeping runtime manageable.
    </p>
    <img src="hyperparam_tune_supervised_nmf.png" alt="NMF Accuracy Curve">
    <p>
        Based on the supervised accuracy curve, convergence appears to occur around 50-70 components. 
        Although this estimate is approximate due to reduced training size and CPU-only computation, it still provides a reasonable range for selecting the optimal dimensionality. 
        Using the full dataset would require polynomial time in the number of components k and is not computationally viable on standard hardware in a reasonable time frame.
    </p>

    <h2> Independent Component Analysis (ICA)</h2>

    <h3> Observations </h3>
    <p>
        Visually, the ICA projection appears similar to the PCA projection, with recognizable clusters corresponding to certain digits such as 0 and 1. 
        This similarity arises because both methods are linear transformations applied to the same data, and the dominant variance structure in MNIST causes the first ICA components to roughly align with the top principal components. 
        However, the underlying principles differ: PCA seeks orthogonal directions that explain maximum variance, while ICA identifies statistically independent directions in the data. 
        Thus, although the scatterplots look comparable, ICA components represent independent pixel intensity sources rather than directions of maximal variance.
    </p>
    
    <img src="ica12_labeled.png" alt="ICA Observation Scatterplot">

    <h3>Features</h3>
    <p>
        The image below corresponds to the independent components learned by ICA. 
        ICA identifies statistically independent contrast patterns rather than purely additive or variance-maximizing directions. 
        Each component contains both positive and negative pixel weights that together describe how certain regions of the image brighten while others darken. 
        These opposing effects capture independent structural contrasts—such as the light outer ring and darker inner region of the digit “0”—that collectively reconstruct the visual patterns present in the dataset.
    </p>
    <img src="ica_feature_patterns.png" alt="ICA Basis Features">
    
    <p>
        The image below shows example digits (observations) that have the highest activation magnitudes for each component. 
        When examining the learned components, Independent Component 2 shows a strong contrast between the inner and outer regions of the digit, corresponding to the circular structure of “0.” 
        This is reflected in the second row of the activation grid, where the most responsive digits are all zeros, illustrating how this independent contrast pattern is expressed in the data.
    </p>
    <img src="ica_top_activations_grid.png" alt="ICA Top Activations">
    

    <h3> Hyperparameter Tuning </h3>
    <p>
        ICA runs significantly faster than NMF because its optimization problem is unconstrained and admits an efficient fixed-point solution.
        As a result, we can test a more exhaustive grid search.
    </p>
    <img src="hyperparam_tune_supervised_ica.png" alt="ICA Accuracy Curve">
    <p>
        Again, similarly to PCA and NMF the components tend around 40 to 60 before convergence. 
    </p>
    
    <h2> Summary </h2>
    <p>
        Across PCA, NMF, and ICA, classification accuracy plateaus between 40 and 60 components. 
        This consistency reflects the intrinsic dimensionality of the MNIST dataset: although each image has 784 pixels, the true underlying structure of handwritten digits can be described by roughly 50 latent factors capturing shape, curvature, and stroke variations. 
        Beyond this point, additional components primarily model noise or redundant detail rather than new discriminative information. 
        Despite differing in formulation - PCA emphasizing variance, NMF enforcing additivity, and ICA seeking independence - all three methods recover a low-dimensional subspace of similar capacity.
    </p>
    <br>
    <p>
        To identify the linear dimension reduction technique that best separates the classes while incorporating a supervised downstream validation, I use the Adjusted Rand Index (ARI). 
        ARI quantifies how well the clusters formed in the reduced feature space align with the true digit labels, adjusted for chance grouping. 
        A higher ARI value indicates stronger agreement between the unsupervised clustering structure and the known class labels, reflecting better class separability.
    </p>
    <img src="ari_barplot.png" alt="ARI Scores">
    <p>
        Among the three linear dimensionality reduction techniques, PCA is the clear winner. 
        It achieved the highest Adjusted Rand Index (ARI), indicating the strongest agreement between the unsupervised cluster structure and the true digit labels. 
        PCA's orthogonal variance-based projection effectively captures the global geometric structure of the MNIST data, producing better digit separation while remaining computationally efficient. 
        NMF provides more interpretable part-based features, and ICA reveals independent contrast patterns, but both yield lower separability scores compared to PCA.
    </p>
        
  </section>

  <section id="nonlinear-methods">
    <h1>(Section B) Non-Linear Dimension Reduction Techniques</h1>

    <h2>Kernel PCA</h2>

    <p>
    Kernel PCA was applied using an RBF kernel (chosen as it generally performs best for nonlinear structure extraction, so the kernel type itself was not treated as a tunable parameter). 
    Due to computational constraints, I used a subsample of the MNIST dataset for hyperparameter tuning. 
    A grid search was conducted over the number of components (<i>r</i>) and the kernel width parameter (γ). 
    For each combination, I fit a multinomial logistic regression model on the transformed features and recorded the test accuracy. 
    The heatmap below shows the results of this supervised downstream evaluation, with the optimal parameter setting highlighted.
    </p>
    
    <img src="kernelpca_hyperparam_heatmap.png" alt="Kernel PCA Hyperparameter Heatmap">
    
    <p>
    Next, I refit Kernel PCA using the best-performing γ value, projecting the data into two dimensions for visualization. 
    This 2D plot is provided purely for qualitative interpretation — the components in Kernel PCA are not nested or variance-ordered, 
    so the first two components do not necessarily represent the most informative directions in the higher-dimensional feature space.
    </p>
    
    <img src="kernelpca12_labeled_2d_tuned.png" alt="Kernel PCA 2D Projection (Tuned)">
    
      <h2>Spectral Embedding</h2>

      <p>
      Spectral Embedding is a graph-based manifold learning method that depends on <code>n_neighbors</code> and <code>n_components</code>. 
      It generally performs best with a small number of neighbors since it preserves <b>local</b> structure rather than global relationships—similarities can only be meaningfully interpreted within each neighborhood. 
      Due to computational constraints, a subsample of the MNIST dataset was used for hyperparameter tuning. 
      A grid search was performed over the number of components and neighbors. 
      For each combination, a multinomial logistic regression model was fit on the embedded data and evaluated on held-out observations. 
      The heatmap below displays the resulting test accuracies from this supervised downstream evaluation, with the optimal setting highlighted.
      </p>
      
      <img src="spectral_hyperparam_heatmap.png" alt="Spectral Embedding Hyperparameter Heatmap">
      
      <p>
      Next, I refit Spectral Embedding using the best-performing <code>n_neighbors</code> value and projected the data into two dimensions for visualization. 
      When the graph was not fully connected, the next-best neighbor value was used to ensure stable eigen-decomposition. 
      The resulting embedding below reveals distinct local groupings, with certain digits—such as 6—showing particularly strong separation.
      </p>
      
      <img src="spectral12_labeled_2d_tuned.png" alt="Spectral Embedding 2D Projection (Tuned)">
        

    <h2> Classical & Metric MDS </h2>

    <p>
      The primary hyperparameter tuned for both MDS variants was the number of components (<code>n_components</code>). 
      Metric MDS proved computationally expensive, taking significantly longer to converge, while Classical MDS—though faster—still achieved stronger downstream classification performance. 
      The optimal dimensionality appeared to be around 40 components, as indicated by the accuracy trend below.
    </p>
    
    <img src="mds_accuracy_comparison.png" alt="MDS Hyperparameter Tuning">
    
    <p>
      The 2D visualizations below show that neither MDS variant produced particularly distinct clusters for the MNIST digits, 
      and the overall fitting process was quite slow. 
      For this dataset, MDS does not seem well-suited compared to other manifold learning methods.
    </p>
    
    <img src="mds_metric_vs_classical_2d.png" alt="MDS 2D Projection (Tuned)">
      

    <h2> t-SNE </h2>

    <p>
    t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear manifold learning technique designed primarily for visualization in two or three dimensions. 
    It models pairwise similarities between points in the high-dimensional space and seeks a low-dimensional embedding that preserves local neighborhood relationships. 
    Because t-SNE is not intended for high-dimensional projections, <code>n_components</code> was fixed to 2 for all experiments to ensure meaningful and stable embeddings suitable for visual interpretation.
    </p>
    
    <p>
    Two key hyperparameters—<code>perplexity</code> and <code>learning_rate</code>—were tuned via grid search. 
    Perplexity controls the effective number of neighbors considered when constructing local probability distributions: small values emphasize very local structure, while larger values capture more global relationships. 
    The learning rate determines the step size during optimization; overly small values may collapse clusters, whereas large values can cause divergence or unstable embeddings. 
    A multinomial logistic regression probe was fit on the resulting embeddings to quantify downstream separability, and accuracy scores were recorded for each hyperparameter combination.
    </p>
    
    <p>
    The figure below presents the heatmap of test accuracies across perplexity-learning rate pairs, with the best-performing setting highlighted. 
    The grid search revealed that t-SNE achieved its highest downstream classification accuracy at a moderate perplexity and learning rate, confirming the method's preference for preserving local neighborhoods over global structure. 
    </p>
    <img src="tsne_hyperparam_heatmap.png" alt="t-SNE 2D Projection (Tuned)">
      
    <p>
      This 2d visual using the tuned hyperparameters from the gridsearch above yield an okay visual representation but does not clearly distinguish clusters. 
      Each class appears to have great seperation.
    </p>
    
    <img src="tsne12_labeled_2d_tuned.png" alt="t-SNE Hyperparameter Tuning Heatmap">
    

    <h2> UMAP </h2>

    <p>
    Uniform Manifold Approximation and Projection (UMAP) is a nonlinear manifold learning algorithm that balances the preservation of both local and global structure in data. 
    Unlike t-SNE, which focuses primarily on local neighborhood preservation, UMAP constructs a weighted graph of nearest neighbors and optimizes a low-dimensional representation that maintains the manifold's overall topology. 
    For this reason, UMAP can reveal broader structural relationships while still achieving sharp local clustering.
    </p>

    <p>
    Two key hyperparameters—<code>n_neighbors</code> and <code>min_dist</code>—were tuned via grid search. 
    The <code>n_neighbors</code> parameter controls the size of the local neighborhood used for manifold approximation; smaller values emphasize fine-grained local structure, while larger values capture more global patterns. 
    The <code>min_dist</code> parameter determines how tightly UMAP packs points together in the low-dimensional embedding—lower values produce more compact clusters, whereas higher values yield smoother, more diffuse representations. 
    Each combination was evaluated using a multinomial logistic classification probe trained on the resulting embeddings to measure downstream classification accuracy.
    </p>

    <p>
    The heatmap below visualizes test accuracies over the <code>(n_neighbors, min_dist)</code> grid, with the optimal configuration highlighted. 
    UMAP achieved its best downstream accuracy with a neighborhood size of 25 and <code>min_dist</code> of 0.3, striking a balance between local continuity and global manifold preservation.
    </p>

    <img src="umap_hyperparam_heatmap.png" alt="UMAP Hyperparameter Tuning Heatmap">

    <p>
      The 2D embedding visualized below corresponds to the tuned hyperparameters from the grid search. 
      The result shows well-separated clusters corresponding to different digits. 
      In a purely unsupervised setting, I find UMAP's 2D representations the most interpretable so far — we see clear grouping of the digits <code>0</code>, <code>1</code>, <code>2</code>, and <code>6</code>, 
      along with merged groupings of digits {<code>4</code>, <code>9</code>, <code>7</code>} and {<code>3</code>, <code>8</code>, <code>5</code>}, 
      which makes intuitive sense since the MNIST hand-drawn digits often exhibit ambiguity between these shapes due to handwriting variation.
    </p>

    <img src="umap12_labeled_2d_tuned.png" alt="UMAP 2D Projection (Tuned)">

    <h2> Autoencoder </h2>

    <p>
      The autoencoder was implemented as a simple two-layer feedforward network with a linear bottleneck layer, 
      trained to minimize reconstruction error using mean squared loss. 
      Unlike the previous manifold learning methods, the autoencoder is an explicit reconstruction-based approach 
      that learns a nonlinear mapping to compress and reconstruct the input data. 
      Given computational constraints, no hyperparameter tuning was performed—training was limited to 200 epochs on CPU 
      with a fixed latent dimensionality of two for visualization.
    </p>
    
    <img src="autoencoder_training_curve.png" alt="Autoencoder Training Curve">
    
    <p>
      The training curve below shows a rapid decrease in reconstruction loss over the first 50 epochs, 
      followed by stable convergence near 0.5 MSE, indicating consistent learning and a well-behaved optimization process.
    </p>
    
    <p>
      The 2D latent representation learned by the encoder is shown below. 
      Although the model was not explicitly optimized for class separation, 
      the visualization reveals some degree of structure in the latent space, 
      with several digits forming partially distinct clusters. 
      However, compared to methods like t-SNE or UMAP, the separation is less pronounced, 
      as the objective focuses purely on reconstruction fidelity rather than neighborhood preservation.
    </p>
    
    <img src="autoencoder_latent_2d.png" alt="Autoencoder Latent 2D Representation">
    
    </section>

    <section id="conclusion">
      <h1>(Part C) Conclusion</h1>
    
      <p>
        Across all dimensionality reduction methods evaluated, the MNIST digit patterns clearly exhibit <b>nonlinear structure</b>. 
        Linear techniques such as PCA and ICA failed to produce meaningful separability in the low-dimensional space, while nonlinear 
        manifold learning methods—particularly <b>UMAP</b> and <b>t-SNE</b>—revealed well-defined digit groupings.
      </p>
    
      <p>
        Among these, <b>UMAP</b> achieved the best overall performance. 
        Visually, it produced the most distinct clusters with clear local neighborhood preservation: 
        digits such as <i>3, 5, and 8</i> were grouped closely together due to their shared curvature and shape, 
        while more isolated digits such as <i>0</i> and <i>1</i> were mapped far apart. 
        This demonstrates UMAP’s strong balance between <b>separability and interpretability</b>—it preserves meaningful local 
        manifold relationships while still maintaining a globally coherent embedding structure.
      </p>
    
      <p>
        Quantitatively, this conclusion aligns with the <b>Adjusted Rand Index (ARI)</b> evaluation using 
        <b>K-means clustering</b> on the 2D embeddings of the tuned models. 
        UMAP achieved the highest ARI (approx 0.51), followed closely by t-SNE (approx 0.47), 
        while spectral methods, kernel PCA, and linear techniques lagged far behind. 
        These results confirm that nonlinear embeddings capture the intrinsic geometry of the MNIST data far more effectively 
        than linear transformations, making <b>UMAP the most suitable choice</b> for this dataset and for similar high-dimensional 
        pattern recognition problems.
      </p>
    
      <img src="ari_comparison_hist.png" alt="ARI Comparison Across Tuned 2D Embeddings">
    </section>

</body>
</html>
