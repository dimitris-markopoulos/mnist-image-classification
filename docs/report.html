<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Unsupervised Machine Learning</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 1000px;
      margin: 40px auto;
      padding: 0 20px;
    }
    h1, h2, h3 {
      color: #222;
    }
    img {
      display: block;
      margin: 20px auto;
      max-width: 90%;
    }
    pre {
      background: #f8f8f8;
      padding: 10px;
      border-left: 3px solid #ccc;
      overflow-x: auto;
    }
    code {
      font-family: Consolas, monospace;
    }
    section {
      margin-bottom: 40px;
    }
  </style>
</head>

<body>

  <h1>Machine Learning: MNIST Digits</h1>

  <section id="data-overview">
    <h1>Data Overview</h1>
    <p>
        The dataset is readily available within <code>sklearn</code>:
    </p>

<pre><code class="language-python">
import pandas as pd
from sklearn.datasets import fetch_openml

mnist = fetch_openml("mnist_784")
X, y = mnist.data, mnist.target.astype(int)
</code></pre>

    <p>
      <code>X</code> is a dataframe containing 784 features, each representing a pixel in a 28 × 28 image. 
      There are 70,000 observations (digit images) in the dataset. 
      Below is a visualization of 25 randomly sampled observations.
    </p>

    <img src="digits_initial_viz.png" alt="Initial visualization of MNIST digits">
  </section>

  <section id="linear-methods">
    <h1>Linear Dimension Reduction Techniques</h1>
    <p>
      This section applies, compares, and contrasts <strong>PCA</strong>, <strong>NMF</strong>, and <strong>ICA</strong>.
    </p>

    <h2>Principal Component Analysis (PCA)</h2>

    <h3> Observations </h3>
    <p>
      PC1 and PC2 capture latent patterns among digits — some digits (e.g., digit 1) are well separated in this projection.
      Interpreting PC1 and PC2 by examining observations along these directions yields the visualization below.
      <img src="pc1_pc2_labeled.png" alt="PCA Scatterplot">
    </p>

    <h3> Features </h3>
    <p>
      PC2 appears to capture <em>digit thickness</em> — moving from positive to negative PC2, digits become thinner.
      PC1 captures a subtler pattern, possibly related to <em>digit slant</em>.
    </p>
    <img src="pc1_pc2_patterns.png" alt="PC Pattern Interpretation">

    <h3> Hyperparameter Tuning </h3>
    The number of principal components (PCs) cannot exceed the number of original features. 
    To determine the optimal number of PCs, we can examine the variance explained using a scree plot and apply the “elbow rule,” 
    or use more quantitative approaches to tune this hyperparameter. 

    Since labels are available, one practical method is to fit a simple multinomial classifier on progressively larger subsets of the principal components 
    and track how classification accuracy changes until it stabilizes. 
    This downstream supervised tuning effectively helps optimize an unsupervised method. 
    Alternatively, we can evaluate the quality of the reduced representation by comparing how well observations in the reduced space preserve relationships 
    from the original space, for example using a Jaccard similarity measure.
    <br><br>
    For PCA, given the analytical closed form, nested, and ordered solution - it makes sense to use supervised optimization. 
    Below I fit 20 mutlinomial classification models on X projected to 5, 10, 15, ..., 100 PCs and evaluate the accuracy of each iteration to inspect convergence.
    </code></pre>
    <img src="hyperparam_tune_supervised_pca.png" alt="PCA Accuracy Curve">

    These results indicate that there is a convergence between 40 to 60 PCs - this appears to be the optimal number of components to use when performing PCA on MNIST dataset. 

    <h2>Non-Negative Matrix Factorization (NMF)</h2>

    <h3> Observations </h3>
    <p>
        Unlike PCA, NMF components are not ordered or nested, so we cannot interpret them sequentially by importance. 
        Below, three arbitrary pairs of components are shown side by side to illustrate how different 2D projections capture various digit groupings without a clear hierarchical structure.
    </p>
    <img src="nmf_1x3_components.png" alt="NMF Observation Scatterplot">
    Because NMF enforces non-negativity, each component contributes additively to the reconstruction of the digits. 
    Unlike PCA, which can capture opposing directions of variation through positive and negative loadings, NMF contains no conflicting variance patterns. 
    This yields more interpretable, part-based components, though at the cost of representing only additive structure in the data.
    Ultimately, the clustering tends to be worse for better interpretation of the patterns.

    <h3> Features </h3>
    <p>
        The image below corresponds to the basis components (the H matrix) learned by NMF. 
        Each component represents a distinct pattern or “building block” that the model has found useful for reconstructing digits. 
        These can be interpreted as localized parts of digits — for example, strokes, loops, or crossbars — that commonly appear across multiple digits.
        The second image shows example digits (observations) that have the highest activation scores for each component, derived from the W matrix. 
        These highlight which digits rely most heavily on each learned part. 
        For instance, component 9 appears to capture the crossbar pattern of the digit 4, while other components align more directly with complete digit shapes such as 1, 0, or 7.
        Overall, NMF decomposes the data into additive, non-negative parts that together reconstruct the full images. 
        While some components resemble entire digits rather than localized features, this behavior is typical for NMF applied to structured image data like MNIST—especially when the number of components is small.
    </p>
    <img src="nmf_basis_features.png" alt="NMF Basis Features">

    <p>
        The image below shows example digits (observations) that have the highest activation scores for each component, derived from the W matrix. 
        These highlight which digits rely most heavily on each learned part. 
        For instance, component 9 appears to capture the crossbar pattern of the digit 4, while other components align more directly with complete digit shapes such as 1, 0, or 7.
        Overall, NMF decomposes the data into additive, non-negative parts that together reconstruct the full images. 
        While some components resemble entire digits rather than localized features, this behavior is typical for NMF applied to structured image data like MNIST—especially when the number of components is small.
    </p>
    <img src="nmf_top_activations_grid.png" alt="NMF Top Activations">
    <h3> Hyperparameter Tuning </h3>
    <p>
        Because NMF must be refit from scratch for each number of components, it is considerably more computationally expensive than PCA. 
        In practice, a coarse grid provides sufficient resolution to identify the accuracy plateau while keeping runtime manageable.
    </p>
    <img src="hyperparam_tune_supervised_nmf.png" alt="NMF Accuracy Curve">
    <p>
        Based on the supervised accuracy curve, convergence appears to occur around 50-70 components. 
        Although this estimate is approximate due to reduced training size and CPU-only computation, it still provides a reasonable range for selecting the optimal dimensionality. 
        Using the full dataset would require polynomial time in the number of components k and is not computationally viable on standard hardware in a reasonable time frame.
    </p>

    <h2> Independent Component Analysis (ICA)</h2>

    <h3> Observations </h3>
    <p>
        Visually, the ICA projection appears similar to the PCA projection, with recognizable clusters corresponding to certain digits such as 0 and 1. 
        This similarity arises because both methods are linear transformations applied to the same data, and the dominant variance structure in MNIST causes the first ICA components to roughly align with the top principal components. 
        However, the underlying principles differ: PCA seeks orthogonal directions that explain maximum variance, while ICA identifies statistically independent directions in the data. 
        Thus, although the scatterplots look comparable, ICA components represent independent pixel intensity sources rather than directions of maximal variance.
    </p>
        
        
    
    <img src="ica12_labeled.png" alt="ICA Observation Scatterplot">

    <h3>Features</h3>
    <p>
        The image below corresponds to the independent components learned by ICA. 
        ICA identifies statistically independent contrast patterns rather than purely additive or variance-maximizing directions. 
        Each component contains both positive and negative pixel weights that together describe how certain regions of the image brighten while others darken. 
        These opposing effects capture independent structural contrasts—such as the light outer ring and darker inner region of the digit “0”—that collectively reconstruct the visual patterns present in the dataset.
    </p>
    <img src="ica_feature_patterns.png" alt="ICA Basis Features">
    
    <p>
        The image below shows example digits (observations) that have the highest activation magnitudes for each component. 
        When examining the learned components, Independent Component 2 shows a strong contrast between the inner and outer regions of the digit, corresponding to the circular structure of “0.” 
        This is reflected in the second row of the activation grid, where the most responsive digits are all zeros, illustrating how this independent contrast pattern is expressed in the data.
    </p>
    <img src="ica_top_activations_grid.png" alt="ICA Top Activations">
    

    <h3> Hyperparameter Tuning </h3>
    <p>
        ICA runs significantly faster than NMF because its optimization problem is unconstrained and admits an efficient fixed-point solution.
        As a result, we can test a more exhaustive grid search.
    </p>
    <img src="hyperparam_tune_supervised_ica.png" alt="ICA Accuracy Curve">
    <p>
        Again, similarly to PCA and NMF the components tend around 40 to 60 before convergence. 
    </p>
    
    <h2> Summary </h2>
    <p>
        Across PCA, NMF, and ICA, classification accuracy plateaus between 40 and 60 components. 
        This consistency reflects the intrinsic dimensionality of the MNIST dataset: although each image has 784 pixels, the true underlying structure of handwritten digits can be described by roughly 50 latent factors capturing shape, curvature, and stroke variations. 
        Beyond this point, additional components primarily model noise or redundant detail rather than new discriminative information. 
        Despite differing in formulation - PCA emphasizing variance, NMF enforcing additivity, and ICA seeking independence - all three methods recover a low-dimensional subspace of similar capacity.
    </p>
    <br>
    <p>
        To identify the linear dimension reduction technique that best separates the classes while incorporating a supervised downstream validation, I use the Adjusted Rand Index (ARI). 
        ARI quantifies how well the clusters formed in the reduced feature space align with the true digit labels, adjusted for chance grouping. 
        A higher ARI value indicates stronger agreement between the unsupervised clustering structure and the known class labels, reflecting better class separability.
    </p>
    <img src="ari_barplot.png" alt="ARI Scores">
    <p>
        Among the three linear dimensionality reduction techniques, PCA is the clear winner. 
        It achieved the highest Adjusted Rand Index (ARI), indicating the strongest agreement between the unsupervised cluster structure and the true digit labels. 
        PCA's orthogonal variance-based projection effectively captures the global geometric structure of the MNIST data, producing better digit separation while remaining computationally efficient. 
        NMF provides more interpretable part-based features, and ICA reveals independent contrast patterns, but both yield lower separability scores compared to PCA.
    </p>
        

        
  </section>

  <section id="nonlinear-methods">
    <h1>Non-Linear Dimension Reduction Techniques</h1>

    <h2>Kernel PCA</h2>

    <p>
    Kernel PCA was applied using an RBF kernel (chosen as it generally performs best for nonlinear structure extraction, so the kernel type itself was not treated as a tunable parameter). 
    Due to computational constraints, I used a subsample of the MNIST dataset for hyperparameter tuning. 
    A grid search was conducted over the number of components (<i>r</i>) and the kernel width parameter (γ). 
    For each combination, I fit a multinomial logistic regression model on the transformed features and recorded the test accuracy. 
    The heatmap below shows the results of this supervised downstream evaluation, with the optimal parameter setting highlighted.
    </p>
    
    <img src="kernelpca_hyperparam_heatmap.png" alt="Kernel PCA Hyperparameter Heatmap">
    
    <p>
    Next, I refit Kernel PCA using the best-performing γ value, projecting the data into two dimensions for visualization. 
    This 2D plot is provided purely for qualitative interpretation — the components in Kernel PCA are not nested or variance-ordered, 
    so the first two components do not necessarily represent the most informative directions in the higher-dimensional feature space.
    </p>
    
    <img src="kernelpca12_labeled_2d_tuned.png" alt="Kernel PCA 2D Projection (Tuned)">
    

    <h2> Spectral Embedding </h2>

    <p>
        ...
    </p>

    <h2> Classical MDS </h2>

    <p>
        ...
    </p>

    <h2> Metric MDS </h2>
    
    <p>
        ...
    </p>

    <h2> t-SNE </h2>

    <p>
        ...
    </p>

    <h2> UMAP </h2>

    <p>
        ...
    </p>

    <h2> Autoencoder </h2>

    <p>
        ...
    </p>

    </section>

    <section id="conclusion">
        <h1>Conclusion</h1>

        <p>
            ...
        </p>
        
    </section>

</body>
</html>
